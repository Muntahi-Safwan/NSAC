version: "3.8"

services:
  data-pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: nsac_data_pipeline
    env_file:
      - .env
    volumes:
      # Mount config files
      - ./.env:/app/.env:ro
      # Mount air-quality module
      - ./air-quality:/app/air-quality
      # Mount heatwave module
      - ./heatwave:/app/heatwave
    # Run pipeline on demand (not as a service)
    profiles:
      - tools
    command: python air-quality/forecast/main.py --sample-rate 5

  # NSAC Data Processing Scheduler (All Pipelines)
  nsac-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: nsac_scheduler
    env_file:
      - .env
    volumes:
      # Persist logs
      - ./logs:/app/logs
      # Mount config files
      - ./.env:/app/.env:ro
      # Mount all modules
      - ./air-quality:/app/air-quality
      - ./heatwave:/app/heatwave
      - ./wildfire:/app/wildfire
      - ./scripts:/app/scripts
    # Run continuous scheduler (hourly execution)
    command: python scripts/run_hourly_collection.py --sample-rate 1
    restart: unless-stopped
    # This service runs continuously and schedules all pipelines hourly

  # Prisma Studio for database management
  prisma-studio:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: prisma_studio
    env_file:
      - .env
    ports:
      - "5555:5555"
    volumes:
      - ./.env:/app/.env:ro
    command: python -m prisma studio --hostname 0.0.0.0 --port 5555
    profiles:
      - tools

# Local postgres service removed - using external database (Neon, AWS RDS, etc.)
# Configure DATABASE_URL in .env file to point to your external database
