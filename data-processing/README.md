# ğŸŒ NSAC Data Processing Pipeline

Modular data processing system for collecting and analyzing environmental data.

## ğŸ“ Directory Structure

```
data-processing/
â”œâ”€â”€ aqi/                    # Air Quality Index data processing
â”‚   â”œâ”€â”€ main.py            # AQI data collection pipeline
â”‚   â”œâ”€â”€ smart_downloader.py # GEOS-CF file downloader
â”‚   â”œâ”€â”€ data_processor.py  # NetCDF data processor
â”‚   â”œâ”€â”€ inspect_netcdf.py  # NetCDF file inspector
â”‚   â”œâ”€â”€ calculator.py      # AQI calculator
â”‚   â”œâ”€â”€ breakpoints.py     # EPA AQI breakpoints
â”‚   â””â”€â”€ README.md          # AQI-specific documentation
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DATA_COLLECTION_STRATEGY.md
â”‚   â””â”€â”€ ... (other docs)
â”œâ”€â”€ downloads/             # Downloaded data files
â”œâ”€â”€ database.py           # Global database connection
â”œâ”€â”€ schema.prisma         # Global database schema
â”œâ”€â”€ requirements.txt      # Global dependencies
â”œâ”€â”€ Dockerfile           # Container setup
â”œâ”€â”€ docker-compose.yml   # Container orchestration
â””â”€â”€ env.example          # Environment template
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Copy environment template
copy env.example .env
# Edit .env with your database credentials
```

### 2. Run with Docker (Recommended)

```bash
# Build and run AQI data collection
docker-compose run --rm data-pipeline
```

### 3. Run Locally

```bash
# Install dependencies
pip install -r requirements.txt

# Run AQI pipeline
cd aqi
python main.py
```

## ğŸ“Š Available Data Processing Units

### ğŸŒ¬ï¸ AQI (Air Quality Index)

- **Location**: `aqi/`
- **Data Source**: GEOS-CF forecast data
- **Coverage**: North America (TEMPO satellite coverage)
- **Parameters**: PM2.5, NO2, O3, SO2, CO, HCHO
- **Output**: Air Quality Index calculations

### ğŸ”¥ Wildfire (Planned)

- **Location**: `wildfire/` (future)
- **Data Source**: TBD
- **Coverage**: TBD

### ğŸŒ¡ï¸ Heatwave (Planned)

- **Location**: `heatwave/` (future)
- **Data Source**: TBD
- **Coverage**: TBD

## ğŸ› ï¸ Development

### Adding New Data Processing Units

1. Create new subfolder (e.g., `wildfire/`)
2. Implement required files:
   - `main.py` - Main pipeline
   - `downloader.py` - Data downloader
   - `processor.py` - Data processor
   - `README.md` - Unit documentation
3. Update global `schema.prisma` if new database tables needed
4. Add to `docker-compose.yml` if containerization needed

### Global Files

- **`database.py`** - Shared database connection and utilities
- **`schema.prisma`** - Database schema (shared across all units)
- **`requirements.txt`** - Python dependencies
- **`Dockerfile`** - Container configuration
- **`docker-compose.yml`** - Container orchestration

## ğŸ“š Documentation

See `docs/` folder for detailed documentation:

- `ARCHITECTURE.md` - System architecture
- `DATA_COLLECTION_STRATEGY.md` - Data collection strategy
- `DOCKER_README.md` - Docker setup guide

## ğŸ”§ Configuration

Environment variables (see `env.example`):

- Database connection settings
- API keys and credentials
- Processing parameters


